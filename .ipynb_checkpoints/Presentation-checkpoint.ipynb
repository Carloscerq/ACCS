{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWqL0xTYDYTX"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Primeiramente, vamos organizar algumas dependencias necessarias para a execucao do jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD_PFY_cE2dc"
   },
   "source": [
    "## Baixando dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzEXyqonDw3X"
   },
   "source": [
    "Para o desenvolvimento do projeto, vamos utilizar as seguintes bibliotecas:\n",
    "\n",
    "- `pandas`: Para conseguir realizar operacoes em cima dos `Dataframes`\n",
    "- `matplotlib`: Para fazer plotagem de graficos\n",
    "- `seaborn`: Para ajudar com a plotagem dos graficos mais comuns\n",
    "- `skops`: Para gerar artefatos a partir dos modelos\n",
    "- `scikit-learn`: Para o treinamento dos modelos\n",
    "- `ydata-profiling`: Para realizar um profiling em cima dos dados, pegando insights\n",
    "- `statsmodels`: Para ajudar com contas\n",
    "- `numpy`: Para ajudar com contas\n",
    "\n",
    "Importando as libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/carlos/.platformio/penv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/carlos/.platformio/penv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: skops in /home/carlos/.platformio/penv/lib/python3.12/site-packages (0.10.0)\n",
      "Requirement already satisfied: seaborn in /home/carlos/.platformio/penv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /home/carlos/.platformio/penv/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: ydata_profiling in /home/carlos/.platformio/penv/lib/python3.12/site-packages (4.8.3)\n",
      "Requirement already satisfied: scikit-learn in /home/carlos/.platformio/penv/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from skops) (0.23.4)\n",
      "Requirement already satisfied: tabulate>=0.8.8 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from skops) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from skops) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy<1.14,>=1.4.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (1.13.1)\n",
      "Requirement already satisfied: pydantic>=2 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (2.8.2)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (6.0.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (3.1.4)\n",
      "Requirement already satisfied: visions<0.7.7,>=0.7.5 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata_profiling) (0.7.6)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (0.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (2.32.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (4.66.4)\n",
      "Requirement already satisfied: multimethod<2,>=1.4 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (1.12)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (0.14.2)\n",
      "Requirement already satisfied: typeguard<5,>=3 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (4.3.0)\n",
      "Requirement already satisfied: imagehash==4.3.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (4.3.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (1.9.3)\n",
      "Requirement already satisfied: dacite>=1.8 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (1.8.1)\n",
      "Requirement already satisfied: numba<1,>=0.56.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from ydata_profiling) (0.60.0)\n",
      "Requirement already satisfied: PyWavelets in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from imagehash==4.3.1->ydata_profiling) (1.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->skops) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->skops) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->skops) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from jinja2<3.2,>=2.11.1->ydata_profiling) (2.1.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from numba<1,>=0.56.0->ydata_profiling) (0.43.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from pydantic>=2->ydata_profiling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from pydantic>=2->ydata_profiling) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata_profiling) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata_profiling) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata_profiling) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata_profiling) (2024.6.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from statsmodels<1,>=0.13.2->ydata_profiling) (0.5.6)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata_profiling) (23.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /home/carlos/.platformio/penv/lib/python3.12/site-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata_profiling) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas skops seaborn matplotlib ydata_profiling scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ewYBV_lPEijH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import skops.io as sio\n",
    "from skops import set_trusted_types\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4ZGtTEIEyWr"
   },
   "source": [
    "## Funcoes auxiliares\n",
    "\n",
    "\n",
    "1 - `plot_corr_graph`: Plota o grafico de correlacao\n",
    "  - Parametros:\n",
    "    - `df` (`pd.DataFrame`): `DataFrame` a ser plotado\n",
    "    - `name` (`str`): Nome do grafico (Opcional)\n",
    "  - Resposta:\n",
    "    - `None`: A funcao nao retorna nenhum valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IaekGnUjE7jF"
   },
   "outputs": [],
   "source": [
    "def plot_corr_graph(df: pd.DataFrame, name: str ='Correlation Matrix') -> None:\n",
    "  corr_matrix = df.corr()\n",
    "  plt.figure(figsize=(50, 50))\n",
    "  sns.heatmap(corr_matrix, cmap='bwr')\n",
    "  plt.title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGagYCDQFlun"
   },
   "source": [
    "2 - `get_top_abs_correlations`: Pega as colunas com as maiores correlacoes\n",
    "  - Parametros:\n",
    "    - `df` (`pd.DataFrame`): `DataFrame` a ser utilizado\n",
    "    - `n` (`int`): Numero de colunas a ser retornados\n",
    "  - Resposta:\n",
    "    - `pd.Series`: Nome das colunas a serem retornadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZOar3F6HFqsT"
   },
   "outputs": [],
   "source": [
    "def get_top_abs_correlations(df: pd.DataFrame, n: int = 5) -> pd.Series:\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "      for j in range(0, i+1):\n",
    "        labels_to_drop.add((cols[i], cols[j]))\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvGdnTCiIEAC"
   },
   "source": [
    "**DEPRECATE**\n",
    "\n",
    "3 - `calculate_precision`: Retorna algumas informacoes sobre o modelo\n",
    "  - Parametros:\n",
    "    - `y_test` (`pd.DataFrame`): `DataFrame` com dados de teste\n",
    "    - `y_pred` (`pd.DataFrame`): `DataFrame` com dados da previsao\n",
    "  - Resposta:\n",
    "    - `None`: Sem nenhuma resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q44pGNGbIGcw"
   },
   "outputs": [],
   "source": [
    "def calculate_precision(y_test: pd.DataFrame, y_pred: pd.DataFrame) -> None:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - `export_model`: Exporta o `skops` dos modelos\n",
    "    - Parametros:\n",
    "        - `model`: Modelo\n",
    "        - `model_name`: Nome a ser salvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model, model_name):\n",
    "    sio.dump(model, model_name + \".skops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HhQ2f3MIp8i"
   },
   "source": [
    "# Importacao dos dados\n",
    "\n",
    "Vamos utilizar o `dengue_sinan.csv` que esta no google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vhCZluHnI_PI"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    #\"/content/drive/MyDrive/ACCS/dengue_sinan.csv\",\n",
    "    #\"/content/dengue_sinan.csv\",\n",
    "    \"./dengue_sinan.csv\",\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qpa6bXTaJI29"
   },
   "source": [
    "Como primeiro passo, vamos pegar alguns insights sobre o que estamos trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718123744982,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "z_2fRimyJT5g",
    "outputId": "a8497889-1c48-43f7-d6be-3a91fdb71742"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620211, 148)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1718123744982,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "szqrvA8xJCb8",
    "outputId": "cde71324-1776-4352-95b8-a85de779c3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 620211 entries, 0 to 620210\n",
      "Columns: 148 entries, NU_NOTIFIC to ID_CNS_SUS_HASHED\n",
      "dtypes: float64(115), int64(8), object(25)\n",
      "memory usage: 700.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3575,
     "status": "ok",
     "timestamp": 1718123748554,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "0r0BArz-JQCu",
    "outputId": "91db4bfb-4cac-47d6-b4fd-54a2db1e6702"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NU_NOTIFIC</th>\n",
       "      <th>TP_NOT</th>\n",
       "      <th>SEM_NOT</th>\n",
       "      <th>NU_ANO</th>\n",
       "      <th>SG_UF_NOT</th>\n",
       "      <th>ID_MUNICIP</th>\n",
       "      <th>ID_REGIONA</th>\n",
       "      <th>ID_UNIDADE</th>\n",
       "      <th>SEM_PRI</th>\n",
       "      <th>SOUNDEX</th>\n",
       "      <th>...</th>\n",
       "      <th>DT_TRANSDM</th>\n",
       "      <th>DT_TRANSRM</th>\n",
       "      <th>DT_TRANSRS</th>\n",
       "      <th>DT_TRANSSE</th>\n",
       "      <th>NU_LOTE_V</th>\n",
       "      <th>NU_LOTE_H</th>\n",
       "      <th>CS_FLXRET</th>\n",
       "      <th>FLXRECEBI</th>\n",
       "      <th>IDENT_MICR</th>\n",
       "      <th>MIGRADO_W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.202110e+05</td>\n",
       "      <td>620211.000000</td>\n",
       "      <td>620211.000000</td>\n",
       "      <td>620211.000000</td>\n",
       "      <td>620211.000000</td>\n",
       "      <td>620211.000000</td>\n",
       "      <td>619972.000000</td>\n",
       "      <td>6.196540e+05</td>\n",
       "      <td>620211.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>620209.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.132800e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.635363e+05</td>\n",
       "      <td>2.000003</td>\n",
       "      <td>202089.455401</td>\n",
       "      <td>2020.719987</td>\n",
       "      <td>29.020938</td>\n",
       "      <td>292045.766950</td>\n",
       "      <td>1390.364726</td>\n",
       "      <td>4.224895e+06</td>\n",
       "      <td>202075.828657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.544726e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.592750e+05</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>267.587171</td>\n",
       "      <td>2.684069</td>\n",
       "      <td>0.694210</td>\n",
       "      <td>6983.781566</td>\n",
       "      <td>64.124777</td>\n",
       "      <td>2.537868e+06</td>\n",
       "      <td>528.878688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.484949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.285162e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>201552.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>110004.000000</td>\n",
       "      <td>1331.000000</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>20211.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.880000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>201922.000000</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>291080.000000</td>\n",
       "      <td>1381.000000</td>\n",
       "      <td>2.505711e+06</td>\n",
       "      <td>201921.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.200500e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202106.000000</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>291800.000000</td>\n",
       "      <td>1385.000000</td>\n",
       "      <td>2.802074e+06</td>\n",
       "      <td>202105.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.291650e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202330.000000</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>292740.000000</td>\n",
       "      <td>1398.000000</td>\n",
       "      <td>6.602533e+06</td>\n",
       "      <td>202328.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.994664e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>202414.000000</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>530010.000000</td>\n",
       "      <td>6255.000000</td>\n",
       "      <td>9.999396e+06</td>\n",
       "      <td>202414.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.927400e+10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         NU_NOTIFIC         TP_NOT        SEM_NOT         NU_ANO  \\\n",
       "count  6.202110e+05  620211.000000  620211.000000  620211.000000   \n",
       "mean   1.635363e+05       2.000003  202089.455401    2020.719987   \n",
       "std    5.592750e+05       0.001796     267.587171       2.684069   \n",
       "min    0.000000e+00       2.000000  201552.000000    2016.000000   \n",
       "25%    7.880000e+02       2.000000  201922.000000    2019.000000   \n",
       "50%    1.200500e+04       2.000000  202106.000000    2021.000000   \n",
       "75%    6.291650e+04       2.000000  202330.000000    2023.000000   \n",
       "max    9.994664e+06       3.000000  202414.000000    2024.000000   \n",
       "\n",
       "           SG_UF_NOT     ID_MUNICIP     ID_REGIONA    ID_UNIDADE  \\\n",
       "count  620211.000000  620211.000000  619972.000000  6.196540e+05   \n",
       "mean       29.020938  292045.766950    1390.364726  4.224895e+06   \n",
       "std         0.694210    6983.781566      64.124777  2.537868e+06   \n",
       "min        11.000000  110004.000000    1331.000000  3.500000e+01   \n",
       "25%        29.000000  291080.000000    1381.000000  2.505711e+06   \n",
       "50%        29.000000  291800.000000    1385.000000  2.802074e+06   \n",
       "75%        29.000000  292740.000000    1398.000000  6.602533e+06   \n",
       "max        53.000000  530010.000000    6255.000000  9.999396e+06   \n",
       "\n",
       "             SEM_PRI  SOUNDEX  ...  DT_TRANSDM  DT_TRANSRM  DT_TRANSRS  \\\n",
       "count  620211.000000      0.0  ...         0.0         0.0         0.0   \n",
       "mean   202075.828657      NaN  ...         NaN         NaN         NaN   \n",
       "std       528.878688      NaN  ...         NaN         NaN         NaN   \n",
       "min     20211.000000      NaN  ...         NaN         NaN         NaN   \n",
       "25%    201921.000000      NaN  ...         NaN         NaN         NaN   \n",
       "50%    202105.000000      NaN  ...         NaN         NaN         NaN   \n",
       "75%    202328.000000      NaN  ...         NaN         NaN         NaN   \n",
       "max    202414.000000      NaN  ...         NaN         NaN         NaN   \n",
       "\n",
       "       DT_TRANSSE  NU_LOTE_V  NU_LOTE_H      CS_FLXRET  FLXRECEBI  \\\n",
       "count         0.0        2.0        0.0  620209.000000        0.0   \n",
       "mean          NaN        0.0        NaN       0.378242        NaN   \n",
       "std           NaN        0.0        NaN       0.484949        NaN   \n",
       "min           NaN        0.0        NaN       0.000000        NaN   \n",
       "25%           NaN        0.0        NaN       0.000000        NaN   \n",
       "50%           NaN        0.0        NaN       0.000000        NaN   \n",
       "75%           NaN        0.0        NaN       1.000000        NaN   \n",
       "max           NaN        0.0        NaN       1.000000        NaN   \n",
       "\n",
       "         IDENT_MICR  MIGRADO_W  \n",
       "count  6.132800e+05        0.0  \n",
       "mean   9.544726e+04        NaN  \n",
       "std    5.285162e+07        NaN  \n",
       "min    2.000000e+00        NaN  \n",
       "25%    4.000000e+00        NaN  \n",
       "50%    4.000000e+00        NaN  \n",
       "75%    4.000000e+00        NaN  \n",
       "max    2.927400e+10        NaN  \n",
       "\n",
       "[8 rows x 123 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EeDxFYeJWS_"
   },
   "source": [
    "Assim, podemos ver que nosso `DataFrame` tem `148` colunas inicialmente com `620211` linhas.\n",
    "\n",
    "## Fazendo a limpeza inicial\n",
    "\n",
    "Como primeiro passo, podemos ver quais sao as colunas que tem **todos** os dados vazios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1718123749419,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "NkCoz6ooJR89",
    "outputId": "6852c78e-8850-4c1f-b0b2-09319d27fd9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOUNDEX esta vazia\n",
      "EVIDENCIA esta vazia\n",
      "CON_FHD esta vazia\n",
      "DT_TRANSUS esta vazia\n",
      "DT_TRANSDM esta vazia\n",
      "DT_TRANSRM esta vazia\n",
      "DT_TRANSRS esta vazia\n",
      "DT_TRANSSE esta vazia\n",
      "NU_LOTE_H esta vazia\n",
      "FLXRECEBI esta vazia\n",
      "MIGRADO_W esta vazia\n"
     ]
    }
   ],
   "source": [
    "nan_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "  if df[col].isnull().all():\n",
    "    nan_cols.append(col)\n",
    "    print(f'{col} esta vazia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HLm4ngxJ0nJ"
   },
   "source": [
    "Assim, podemos remover essas colunas que estao vazias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_6uZ0dXrJ5As"
   },
   "outputs": [],
   "source": [
    "df.drop(nan_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jNhmrLUKCar"
   },
   "source": [
    "Depois disso, podemos nos livrar das colunas que tenham apenas valores constantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1332,
     "status": "ok",
     "timestamp": 1718123750740,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "qi9EewFgJ6mE",
    "outputId": "9b7bebed-ef49-4188-dff4-234e25cc99c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_AGRAVO apresenta valores constantes\n"
     ]
    }
   ],
   "source": [
    "const_cols = df.columns[df.nunique(dropna=False) <= 1]\n",
    "\n",
    "for col in const_cols:\n",
    "  print(f'{col} apresenta valores constantes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC2j5fBrKRAs"
   },
   "source": [
    "Deletando essas tabelas com valores constante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "k0uA0ZXuKO0c"
   },
   "outputs": [],
   "source": [
    "df.drop(const_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_aVq478KX10"
   },
   "source": [
    "Podemos ver, agora, quantas tabelas removemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1718123751060,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "QURtUSl7KW4S",
    "outputId": "253ede32-0e9a-4c7f-ae9a-7d3e65cd5a66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620211, 136)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFNHK8TLKtNu"
   },
   "source": [
    "# Gerando relatorio\n",
    "\n",
    "Podemos utilizar o `ydata` para gerar um relatorio inicial sobre os nossos dados. Assim, geramos um `report.html` que pode conter informacoes importantes.\n",
    "\n",
    "**Atencao**: Essa celula pode demorar alguns minutos para rodar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "g0yHwR6dKexQ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21f44bfeed342faaf8c6c1d436af374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/correlations.py:66: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
      "To hide this warning, disable the calculation\n",
      "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
      "If this is problematic for your use case, please report this as an issue:\n",
      "https://github.com/ydataai/ydata-profiling/issues\n",
      "(include the error message: 'could not convert string to float: 'M'')\n",
      "  warnings.warn(\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n",
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/ydata_profiling/model/pandas/duplicates_pandas.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  .reset_index(name=duplicates_key)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c93c0b185ad4c6bb9efce5b290a7e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bac8fb2a90e4bfb8170bfa1cb9026b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6895264379a14394a37fb13bde4e29d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile.to_file('report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwSrMDQWRDsw"
   },
   "source": [
    "A partir do que investigamos no recurso a cima, podemos remover as seguintes colunas:\n",
    "\n",
    "- NU_LOTE_V: Apenas um valor nao nulo\n",
    "- NU_LOTE_I: Apenas um valor nao nulo\n",
    "- SG_UF: Todos os valores nao nulos iguais\n",
    "- ID_PAIS: Todos os valores nao nulos iguais\n",
    "- GENGIVO: Apenas dois valores nao nulos\n",
    "- METRO: Apenas dois valores nao nulos\n",
    "- SANGRAM: Apenas dois valores nao nulos\n",
    "- TP_NOT: Apenas um valor diferente dos demais\n",
    "- SG_UF: Todos os valores nao nulos iguais\n",
    "- ID_PAIS: Todos os valores nao nulos iguais\n",
    "- TP_SISTEMA: Todos os valores nao nulos iguais\n",
    "- NDUPLIC_N: Todos os valores nao nulos iguais\n",
    "- DT_TRANSSM: Todos os valores nao nulos iguais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SiPyBCpcReZz"
   },
   "outputs": [],
   "source": [
    "df.drop([\n",
    "    'TP_NOT',\n",
    "    'SG_UF',\n",
    "    'ID_PAIS',\n",
    "    'NU_LOTE_I',\n",
    "    'TP_SISTEMA',\n",
    "    'NDUPLIC_N',\n",
    "    'DT_TRANSSM',\n",
    "    'NU_LOTE_V',\n",
    "    'GENGIVO',\n",
    "    'METRO',\n",
    "    'SANGRAM'\n",
    "], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf7Tth2KLreJ"
   },
   "source": [
    "# Transformando valores\n",
    "\n",
    "Vamos transformar valores entre `str` para `int` e `datetime`.\n",
    "\n",
    "- Para `datetime`:\n",
    "  - DT_SIN_PRI\n",
    "  - ID_OCUPA_N\n",
    "  - DT_INVEST\n",
    "  - DT_DIGITA\n",
    "  - DT_NOTIFIC\n",
    "  - DT_CHIK_S1\n",
    "  - DT_CHIK_S2\n",
    "  - DT_PRNT\n",
    "  - DT_SORO\n",
    "  - DT_NS1\n",
    "  - DT_VIRAL\n",
    "  - DT_INTERNA\n",
    "  - DT_OBITO\n",
    "  - DT_ALRM\n",
    "  - DT_GRAV\n",
    "  - DT_PCR\n",
    "  - DT_ENCERRA\n",
    "  - DT_TRANSSM\n",
    "\n",
    "- Para `int`:\n",
    "  - ID_OCUPA_N\n",
    "  - CS_SEXO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1281,
     "status": "ok",
     "timestamp": 1718123752667,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "nAlb2fGvLDYr",
    "outputId": "04f5c927-4d24-498b-fe94-a4ddcf1f6962"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5086/833572816.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "cols = [\n",
    "    'DT_SIN_PRI',\n",
    "    'ID_OCUPA_N',\n",
    "    'DT_INVEST',\n",
    "    'DT_DIGITA',\n",
    "    'DT_NOTIFIC',\n",
    "    'DT_CHIK_S1',\n",
    "    'DT_CHIK_S2',\n",
    "    'DT_PRNT',\n",
    "    'DT_SORO',\n",
    "    'DT_NS1',\n",
    "    'DT_VIRAL',\n",
    "    'DT_INTERNA',\n",
    "    'DT_OBITO',\n",
    "    'DT_ALRM',\n",
    "    'DT_GRAV',\n",
    "    'DT_PCR',\n",
    "    'DT_ENCERRA'\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "  df[col] = pd.to_datetime(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xX4bWkYSMa9G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5086/3982873281.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['CS_SEXO'] = df['CS_SEXO'].replace({'M': 1, 'F': 0, 'I': 2})\n"
     ]
    }
   ],
   "source": [
    "df['ID_OCUPA_N'] = pd.to_numeric(df['ID_OCUPA_N'], errors='coerce')\n",
    "df['CS_SEXO'] = df['CS_SEXO'].replace({'M': 1, 'F': 0, 'I': 2})\n",
    "df['NM_REFEREN'] = pd.factorize(df['NM_REFEREN'])[0]\n",
    "df['NM_BAIRRO'] = pd.factorize(df['NM_BAIRRO'])[0]\n",
    "df['NOBAIINF'] = pd.factorize(df['NOBAIINF'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9L6AdTSMsAE"
   },
   "source": [
    "Removendo colunas de `str`\n",
    "\n",
    "- DS_OBS : Observacoes sobre o caso\n",
    "- ID_CNS_SUS_HASHED : ID Sus\n",
    "\n",
    "Podemos remover essa tabela por nao serem dados que vamos poder utilizar para o nosso desenvolvimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xcAvOtwkMcaA"
   },
   "outputs": [],
   "source": [
    "df.drop([\n",
    "    'DS_OBS',\n",
    "    'ID_CNS_SUS_HASHED'\n",
    "], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9InTHAusN93G"
   },
   "source": [
    "# Investigando\n",
    "\n",
    "## Corelacao\n",
    "\n",
    "Podemos comecar realizando a correlacao entre as variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23482,
     "status": "ok",
     "timestamp": 1718123776694,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "Y3pYMO7ROL58",
    "outputId": "903cd5c5-b445-456f-e140-5b1930359c6f"
   },
   "outputs": [],
   "source": [
    "plot_corr_graph(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5C6O850QTWO"
   },
   "source": [
    "Visualizando como tabela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16997,
     "status": "ok",
     "timestamp": 1718123793675,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "UtvugG93OMl1",
    "outputId": "f0f92264-c42e-4741-efc3-93dfd0f80e6f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NU_NOTIFIC</th>\n",
       "      <th>DT_NOTIFIC</th>\n",
       "      <th>SEM_NOT</th>\n",
       "      <th>NU_ANO</th>\n",
       "      <th>SG_UF_NOT</th>\n",
       "      <th>ID_MUNICIP</th>\n",
       "      <th>ID_REGIONA</th>\n",
       "      <th>ID_UNIDADE</th>\n",
       "      <th>DT_SIN_PRI</th>\n",
       "      <th>SEM_PRI</th>\n",
       "      <th>...</th>\n",
       "      <th>EPISTAXE</th>\n",
       "      <th>PETEQUIAS</th>\n",
       "      <th>HEMATURA</th>\n",
       "      <th>LACO_N</th>\n",
       "      <th>PLASMATICO</th>\n",
       "      <th>PLAQ_MENOR</th>\n",
       "      <th>COMPLICA</th>\n",
       "      <th>DT_DIGITA</th>\n",
       "      <th>CS_FLXRET</th>\n",
       "      <th>IDENT_MICR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NU_NOTIFIC</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031313</td>\n",
       "      <td>0.029076</td>\n",
       "      <td>0.026630</td>\n",
       "      <td>0.220976</td>\n",
       "      <td>0.223496</td>\n",
       "      <td>0.105601</td>\n",
       "      <td>-0.026054</td>\n",
       "      <td>0.021533</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.497249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT_NOTIFIC</th>\n",
       "      <td>0.031313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999295</td>\n",
       "      <td>0.996920</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>0.013376</td>\n",
       "      <td>0.086760</td>\n",
       "      <td>0.744571</td>\n",
       "      <td>0.509323</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.862609</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.047618</td>\n",
       "      <td>-0.229100</td>\n",
       "      <td>-0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEM_NOT</th>\n",
       "      <td>0.029076</td>\n",
       "      <td>0.999295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.016088</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.084979</td>\n",
       "      <td>0.744096</td>\n",
       "      <td>0.509636</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.047573</td>\n",
       "      <td>-0.230101</td>\n",
       "      <td>-0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NU_ANO</th>\n",
       "      <td>0.026630</td>\n",
       "      <td>0.996920</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.015716</td>\n",
       "      <td>0.015036</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.742367</td>\n",
       "      <td>0.509149</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047498</td>\n",
       "      <td>-0.230737</td>\n",
       "      <td>-0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SG_UF_NOT</th>\n",
       "      <td>0.220976</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988639</td>\n",
       "      <td>0.536426</td>\n",
       "      <td>-0.010616</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>0.005276</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001820</td>\n",
       "      <td>0.034225</td>\n",
       "      <td>-0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAQ_MENOR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMPLICA</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT_DIGITA</th>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.047618</td>\n",
       "      <td>0.047573</td>\n",
       "      <td>0.047498</td>\n",
       "      <td>-0.001820</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.037301</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.936766</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030260</td>\n",
       "      <td>-0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CS_FLXRET</th>\n",
       "      <td>0.003735</td>\n",
       "      <td>-0.229100</td>\n",
       "      <td>-0.230101</td>\n",
       "      <td>-0.230737</td>\n",
       "      <td>0.034225</td>\n",
       "      <td>0.023313</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>-0.167251</td>\n",
       "      <td>-0.116386</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.030260</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDENT_MICR</th>\n",
       "      <td>0.001026</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000575</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-0.030456</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            NU_NOTIFIC  DT_NOTIFIC   SEM_NOT    NU_ANO  SG_UF_NOT  ID_MUNICIP  \\\n",
       "NU_NOTIFIC    1.000000    0.031313  0.029076  0.026630   0.220976    0.223496   \n",
       "DT_NOTIFIC    0.031313    1.000000  0.999295  0.996920   0.009270    0.016407   \n",
       "SEM_NOT       0.029076    0.999295  1.000000  0.999143   0.009248    0.016088   \n",
       "NU_ANO        0.026630    0.996920  0.999143  1.000000   0.009226    0.015716   \n",
       "SG_UF_NOT     0.220976    0.009270  0.009248  0.009226   1.000000    0.988639   \n",
       "...                ...         ...       ...       ...        ...         ...   \n",
       "PLAQ_MENOR    1.000000   -1.000000 -1.000000       NaN        NaN    1.000000   \n",
       "COMPLICA     -1.000000   -1.000000 -1.000000       NaN        NaN   -1.000000   \n",
       "DT_DIGITA     0.002047    0.047618  0.047573  0.047498  -0.001820   -0.000126   \n",
       "CS_FLXRET     0.003735   -0.229100 -0.230101 -0.230737   0.034225    0.023313   \n",
       "IDENT_MICR    0.001026   -0.000261 -0.000204 -0.000144  -0.000054    0.000161   \n",
       "\n",
       "            ID_REGIONA  ID_UNIDADE  DT_SIN_PRI   SEM_PRI  ...  EPISTAXE  \\\n",
       "NU_NOTIFIC    0.105601   -0.026054    0.021533  0.010667  ...      -1.0   \n",
       "DT_NOTIFIC    0.013376    0.086760    0.744571  0.509323  ...       1.0   \n",
       "SEM_NOT       0.014159    0.084979    0.744096  0.509636  ...       1.0   \n",
       "NU_ANO        0.015036    0.083000    0.742367  0.509149  ...       NaN   \n",
       "SG_UF_NOT     0.536426   -0.010616    0.007662  0.005276  ...       NaN   \n",
       "...                ...         ...         ...       ...  ...       ...   \n",
       "PLAQ_MENOR    1.000000    1.000000   -1.000000 -1.000000  ...      -1.0   \n",
       "COMPLICA     -1.000000   -1.000000   -1.000000 -1.000000  ...       NaN   \n",
       "DT_DIGITA     0.002813    0.000703    0.037301  0.024354  ...       1.0   \n",
       "CS_FLXRET     0.019867    0.009835   -0.167251 -0.116386  ...      -1.0   \n",
       "IDENT_MICR   -0.000095   -0.000575   -0.000135 -0.000060  ...       NaN   \n",
       "\n",
       "            PETEQUIAS  HEMATURA  LACO_N  PLASMATICO  PLAQ_MENOR  COMPLICA  \\\n",
       "NU_NOTIFIC        1.0       1.0     1.0   -0.497249         1.0      -1.0   \n",
       "DT_NOTIFIC       -1.0      -1.0    -1.0    0.862609        -1.0      -1.0   \n",
       "SEM_NOT          -1.0      -1.0    -1.0    0.866025        -1.0      -1.0   \n",
       "NU_ANO            NaN       NaN     NaN         NaN         NaN       NaN   \n",
       "SG_UF_NOT         NaN       NaN     NaN         NaN         NaN       NaN   \n",
       "...               ...       ...     ...         ...         ...       ...   \n",
       "PLAQ_MENOR        1.0       1.0     1.0   -1.000000         1.0       NaN   \n",
       "COMPLICA          NaN       NaN     NaN         NaN         NaN       1.0   \n",
       "DT_DIGITA        -1.0      -1.0    -1.0    0.936766        -1.0      -1.0   \n",
       "CS_FLXRET         1.0       1.0     1.0   -1.000000         1.0       1.0   \n",
       "IDENT_MICR        NaN       NaN     NaN         NaN         NaN       NaN   \n",
       "\n",
       "            DT_DIGITA  CS_FLXRET  IDENT_MICR  \n",
       "NU_NOTIFIC   0.002047   0.003735    0.001026  \n",
       "DT_NOTIFIC   0.047618  -0.229100   -0.000261  \n",
       "SEM_NOT      0.047573  -0.230101   -0.000204  \n",
       "NU_ANO       0.047498  -0.230737   -0.000144  \n",
       "SG_UF_NOT   -0.001820   0.034225   -0.000054  \n",
       "...               ...        ...         ...  \n",
       "PLAQ_MENOR  -1.000000   1.000000         NaN  \n",
       "COMPLICA    -1.000000   1.000000         NaN  \n",
       "DT_DIGITA    1.000000  -0.030260   -0.000261  \n",
       "CS_FLXRET   -0.030260   1.000000   -0.030456  \n",
       "IDENT_MICR  -0.000261  -0.030456    1.000000  \n",
       "\n",
       "[123 rows x 123 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_UcNWNaV-se"
   },
   "source": [
    "Podemos utilizar `get_top_abs_correlations` para pegar as correlacoes com maior valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18769,
     "status": "ok",
     "timestamp": 1718123812430,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "MNdXa1UwQVEo",
    "outputId": "8e34305f-69f4-417c-a003-42af6379c5e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMUNINF    MANI_HEMOR    1.0\n",
      "ID_RG_RESI  PLASMATICO    1.0\n",
      "ID_REGIONA  PLASMATICO    1.0\n",
      "DT_PCR      PETEQUIAS     1.0\n",
      "NM_BAIRRO   EPISTAXE      1.0\n",
      "ID_MUNICIP  ID_LOGRADO    1.0\n",
      "ID_UNIDADE  PETEQUIAS     1.0\n",
      "            EPISTAXE      1.0\n",
      "NM_BAIRRO   PETEQUIAS     1.0\n",
      "ID_MUNICIP  HEMATURA      1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(get_top_abs_correlations(df, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej8kEuryWQt2"
   },
   "source": [
    "Podemos, agora, para automatizar o processo, gerar uma lista com esses valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21061,
     "status": "ok",
     "timestamp": 1718123833469,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "gqGspdBdWFFM",
    "outputId": "02208d7a-5d22-4bd2-da09-31c24145cdb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEM_NOT', 'NU_ANO', 'ID_MUNICIP', 'ID_DISTRIT', 'ID_LOGRADO', 'ID_GEO2', 'NM_REFEREN', 'DT_INVEST', 'MIALGIA', 'EXANTEMA', 'NAUSEA', 'DOR_COSTAS', 'ARTRALGIA', 'DT_CHIK_S1', 'DT_SORO', 'RESUL_PCR_', 'SOROTIPO', 'MUNICIPIO', 'COMUNINF', 'CODISINF', 'CRITERIO', 'EVOLUCAO', 'DT_ENCERRA', 'ALRM_ABDOM', 'ALRM_LETAR', 'ALRM_LIQ', 'GRAV_ENCH', 'GRAV_TAQUI', 'GRAV_EXTRE', 'GRAV_CONSC', 'GRAV_ORGAO', 'MANI_HEMOR', 'EPISTAXE', 'PETEQUIAS', 'HEMATURA', 'LACO_N', 'PLASMATICO', 'PLAQ_MENOR', 'COMPLICA', 'DT_DIGITA', 'CS_FLXRET']\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = df.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# Vamos filtrar tabelas com correlacao maior que 95%\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocH2KmwsWdJK"
   },
   "source": [
    "Assim, podemos remover as seguintes colunas:\n",
    "- SEM_NOT\n",
    "- NU_ANO\n",
    "- ID_MUNICIP\n",
    "- ID_DISTRIT\n",
    "- ID_LOGRADO\n",
    "- DT_INVEST\n",
    "- MIALGIA\n",
    "- EXANTEMA\n",
    "- NAUSEA\n",
    "- DOR_COSTAS\n",
    "- ARTRALGIA\n",
    "- DT_CHIK_S1\n",
    "- DT_SORO\n",
    "- RESUL_PCR_\n",
    "- SOROTIPO\n",
    "- MUNICIPIO\n",
    "- HOSPITAL\n",
    "- COMUNINF\n",
    "- CODISINF\n",
    "- CRITERIO\n",
    "- EVOLUCAO\n",
    "- DT_ENCERRA\n",
    "- ALRM_ABDOM\n",
    "- ALRM_LETAR\n",
    "- ALRM_LIQ\n",
    "- GRAV_ENCH\n",
    "- GRAV_TAQUI\n",
    "- GRAV_EXTRE\n",
    "- GRAV_CONSC\n",
    "- GRAV_ORGAO\n",
    "- MANI_HEMOR\n",
    "- EPISTAXE\n",
    "- PETEQUIAS\n",
    "- HEMATURA\n",
    "- LACO_N\n",
    "- PLASMATICO\n",
    "- PLAQ_MENOR\n",
    "- COMPLICA\n",
    "- DT_DIGITA\n",
    "- CS_FLXRET\n",
    "- UF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "i7pTnRh8WY2U"
   },
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "  'SEM_NOT',\n",
    "  'NU_ANO',\n",
    "  'ID_MUNICIP',\n",
    "  'ID_DISTRIT',\n",
    "  'ID_LOGRADO',\n",
    "  'ID_GEO2',\n",
    "  'DT_INVEST',\n",
    "  'MIALGIA',\n",
    "  'EXANTEMA',\n",
    "  'NAUSEA',\n",
    "  'DOR_COSTAS',\n",
    "  'ARTRALGIA',\n",
    "  'DT_CHIK_S1',\n",
    "  'DT_SORO',\n",
    "  'RESUL_PCR_',\n",
    "  'SOROTIPO',\n",
    "  'MUNICIPIO',\n",
    "  'COMUNINF',\n",
    "  'CODISINF',\n",
    "  'CRITERIO',\n",
    "  'EVOLUCAO',\n",
    "  'DT_ENCERRA',\n",
    "  'ALRM_ABDOM',\n",
    "  'ALRM_LETAR',\n",
    "  'ALRM_LIQ',\n",
    "  'GRAV_ENCH',\n",
    "  'GRAV_TAQUI',\n",
    "  'GRAV_EXTRE',\n",
    "  'GRAV_CONSC',\n",
    "  'GRAV_ORGAO',\n",
    "  'MANI_HEMOR',\n",
    "  'EPISTAXE',\n",
    "  'PETEQUIAS',\n",
    "  'HEMATURA',\n",
    "  'LACO_N',\n",
    "  'PLASMATICO',\n",
    "  'PLAQ_MENOR',\n",
    "  'COMPLICA',\n",
    "  'DT_DIGITA',\n",
    "  'CS_FLXRET',\n",
    "  'HOSPITAL',\n",
    "  'UF'\n",
    "]\n",
    "\n",
    "df.drop(to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15459,
     "status": "ok",
     "timestamp": 1718123848898,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "5KAZJRaZWgNf",
    "outputId": "f9d2c2db-b80e-41df-d8cc-358ababfbc6f"
   },
   "outputs": [],
   "source": [
    "plot_corr_graph(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H_c__TOW0sH"
   },
   "source": [
    "# Tratando valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1718123848899,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "e-wwOgVjWjuN",
    "outputId": "3783e058-017e-40e0-b53b-d6c0910b4582"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NU_NOTIFIC', 'DT_NOTIFIC', 'SG_UF_NOT', 'ID_REGIONA', 'ID_UNIDADE',\n",
       "       'DT_SIN_PRI', 'SEM_PRI', 'NU_IDADE_N', 'CS_SEXO', 'CS_GESTANT',\n",
       "       'CS_RACA', 'CS_ESCOL_N', 'ID_MN_RESI', 'ID_RG_RESI', 'ID_BAIRRO',\n",
       "       'NM_BAIRRO', 'ID_GEO1', 'NM_REFEREN', 'CS_ZONA', 'ID_OCUPA_N', 'FEBRE',\n",
       "       'CEFALEIA', 'VOMITO', 'CONJUNTVIT', 'ARTRITE', 'PETEQUIA_N',\n",
       "       'LEUCOPENIA', 'LACO', 'DOR_RETRO', 'DIABETES', 'HEMATOLOG', 'HEPATOPAT',\n",
       "       'RENAL', 'HIPERTENSA', 'ACIDO_PEPT', 'AUTO_IMUNE', 'DT_CHIK_S2',\n",
       "       'DT_PRNT', 'RES_CHIKS1', 'RES_CHIKS2', 'RESUL_PRNT', 'RESUL_SORO',\n",
       "       'DT_NS1', 'RESUL_NS1', 'DT_VIRAL', 'RESUL_VI_N', 'DT_PCR', 'HISTOPA_N',\n",
       "       'IMUNOH_N', 'HOSPITALIZ', 'DT_INTERNA', 'DDD_HOSP', 'TEL_HOSP',\n",
       "       'TPAUTOCTO', 'COUFINF', 'COPAISINF', 'CO_BAINF', 'NOBAIINF',\n",
       "       'CLASSI_FIN', 'DOENCA_TRA', 'CLINC_CHIK', 'DT_OBITO', 'ALRM_HIPOT',\n",
       "       'ALRM_PLAQ', 'ALRM_VOM', 'ALRM_SANG', 'ALRM_HEMAT', 'ALRM_HEPAT',\n",
       "       'DT_ALRM', 'GRAV_PULSO', 'GRAV_CONV', 'GRAV_INSUF', 'GRAV_HIPOT',\n",
       "       'GRAV_HEMAT', 'GRAV_MELEN', 'GRAV_METRO', 'GRAV_SANG', 'GRAV_AST',\n",
       "       'GRAV_MIOC', 'DT_GRAV', 'IDENT_MICR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb-zXWCEW6Tu"
   },
   "source": [
    "Inicialmente, podemos ver quais tabelas do nosso `DataFrame` apresentam valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1718123848899,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "rktdUbPNW4UN",
    "outputId": "f94a93fc-5cb2-4d4d-c201-38a0276973d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NU_NOTIFIC         0\n",
       "DT_NOTIFIC         0\n",
       "SG_UF_NOT          0\n",
       "ID_REGIONA       239\n",
       "ID_UNIDADE       557\n",
       "               ...  \n",
       "GRAV_SANG     619633\n",
       "GRAV_AST      619634\n",
       "GRAV_MIOC     619633\n",
       "DT_GRAV       619671\n",
       "IDENT_MICR      6931\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7At1ueAXEIN"
   },
   "source": [
    "Listando as tabelas que apresentam valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1718123848900,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "ctStWBq_W_st",
    "outputId": "8ff8dbb6-0837-4dfc-fd80-390cc8ea2929"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID_REGIONA',\n",
       " 'ID_UNIDADE',\n",
       " 'DT_SIN_PRI',\n",
       " 'CS_SEXO',\n",
       " 'CS_GESTANT',\n",
       " 'CS_RACA',\n",
       " 'CS_ESCOL_N',\n",
       " 'ID_RG_RESI',\n",
       " 'ID_BAIRRO',\n",
       " 'ID_GEO1',\n",
       " 'CS_ZONA',\n",
       " 'FEBRE',\n",
       " 'CEFALEIA',\n",
       " 'VOMITO',\n",
       " 'CONJUNTVIT',\n",
       " 'ARTRITE',\n",
       " 'PETEQUIA_N',\n",
       " 'LEUCOPENIA',\n",
       " 'LACO',\n",
       " 'DOR_RETRO',\n",
       " 'DIABETES',\n",
       " 'HEMATOLOG',\n",
       " 'HEPATOPAT',\n",
       " 'RENAL',\n",
       " 'HIPERTENSA',\n",
       " 'ACIDO_PEPT',\n",
       " 'AUTO_IMUNE',\n",
       " 'DT_CHIK_S2',\n",
       " 'DT_PRNT',\n",
       " 'RES_CHIKS1',\n",
       " 'RES_CHIKS2',\n",
       " 'RESUL_PRNT',\n",
       " 'RESUL_SORO',\n",
       " 'DT_NS1',\n",
       " 'RESUL_NS1',\n",
       " 'DT_VIRAL',\n",
       " 'RESUL_VI_N',\n",
       " 'DT_PCR',\n",
       " 'HISTOPA_N',\n",
       " 'IMUNOH_N',\n",
       " 'HOSPITALIZ',\n",
       " 'DT_INTERNA',\n",
       " 'DDD_HOSP',\n",
       " 'TEL_HOSP',\n",
       " 'TPAUTOCTO',\n",
       " 'COUFINF',\n",
       " 'COPAISINF',\n",
       " 'CO_BAINF',\n",
       " 'CLASSI_FIN',\n",
       " 'DOENCA_TRA',\n",
       " 'CLINC_CHIK',\n",
       " 'DT_OBITO',\n",
       " 'ALRM_HIPOT',\n",
       " 'ALRM_PLAQ',\n",
       " 'ALRM_VOM',\n",
       " 'ALRM_SANG',\n",
       " 'ALRM_HEMAT',\n",
       " 'ALRM_HEPAT',\n",
       " 'DT_ALRM',\n",
       " 'GRAV_PULSO',\n",
       " 'GRAV_CONV',\n",
       " 'GRAV_INSUF',\n",
       " 'GRAV_HIPOT',\n",
       " 'GRAV_HEMAT',\n",
       " 'GRAV_MELEN',\n",
       " 'GRAV_METRO',\n",
       " 'GRAV_SANG',\n",
       " 'GRAV_AST',\n",
       " 'GRAV_MIOC',\n",
       " 'DT_GRAV',\n",
       " 'IDENT_MICR']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWCHe2hgXKWN"
   },
   "source": [
    "Para a coluna `CLASSI_FIN`, vamos remover aquelas que apresentam valor nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Z8trAUmGXGiK"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['CLASSI_FIN'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q8fagfSXQVl"
   },
   "source": [
    "Depois disso, podemos utilizar o `replace` para transformar os diversos valores em 0 caso o paciente nao tenha dengue e 1 caso o mesmo tenha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1718123849301,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "aJ9FRLtwXP21",
    "outputId": "72787fb3-134b-4464-e4fe-e66bfc3a5d9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8., 10.,  5.,  1.,  2., 12., 11.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CLASSI_FIN'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "H5OmRq_CXYA5"
   },
   "outputs": [],
   "source": [
    "df['CLASSI_FIN'] = df['CLASSI_FIN'].replace(5, 0)\n",
    "df['CLASSI_FIN'] = df['CLASSI_FIN'].replace(8, 0)\n",
    "df['CLASSI_FIN'] = df['CLASSI_FIN'].replace(1, 0)\n",
    "df['CLASSI_FIN'] = df['CLASSI_FIN'].replace(2, 0)\n",
    "df['CLASSI_FIN'] = df['CLASSI_FIN'].replace(10, 1)\n",
    "df['CLASSI_FIN'] = df['CLASSI_FIN'].replace(11, 1)\n",
    "df['CLASSI_FIN'] = df['CLASSI_FIN'].replace(12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hb1OmHeBXvhU"
   },
   "source": [
    "Vamos agora tratar os campos restantes de forma individual\n",
    "\n",
    "Para os campos:\n",
    "\n",
    "- ID_REGIONA\n",
    "- ID_UNIDADE\n",
    "- ID_RG_RESI\n",
    "- ID_BAIRRO\n",
    "- DDD_HOSP\n",
    "- TEL_HOSP\n",
    "- ID_GEO1\n",
    "- CS_ZONA\n",
    "- COUFINF\n",
    "\n",
    "Vamos preencher utilizando como base, os valores nao nulos da tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tTVQSFslXxpH"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "  'ID_REGIONA',\n",
    "  'ID_UNIDADE',\n",
    "  'ID_RG_RESI',\n",
    "  'ID_BAIRRO',\n",
    "  'DDD_HOSP',\n",
    "  'TEL_HOSP',\n",
    "  'ID_GEO1',\n",
    "  'CS_ZONA',\n",
    "  'COUFINF'\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "  df[col] = df[col].ffill()\n",
    "  df[col] = df[col].bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwQNua5kYN21"
   },
   "source": [
    "Para os campos de sintomas, vamos colocar o valor `2` como padrao, ja que representa o nao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tRvRlKywYSBA"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "  \"FEBRE\",\n",
    "  \"CEFALEIA\",\n",
    "  \"VOMITO\",\n",
    "  \"CONJUNTVIT\",\n",
    "  \"ARTRITE\",\n",
    "  \"PETEQUIA_N\",\n",
    "  \"LEUCOPENIA\",\n",
    "  \"LACO\",\n",
    "  \"DOR_RETRO\",\n",
    "  \"CLASSI_FIN\",\n",
    "  \"DIABETES\",\n",
    "  \"HEMATOLOG\",\n",
    "  'HEPATOPAT',\n",
    "  'RENAL',\n",
    "  'HIPERTENSA',\n",
    "  'ACIDO_PEPT',\n",
    "  'AUTO_IMUNE',\n",
    "  'GRAV_PULSO',\n",
    "  'GRAV_CONV',\n",
    "  'GRAV_INSUF',\n",
    "  'GRAV_HIPOT',\n",
    "  'GRAV_HEMAT',\n",
    "  'GRAV_MELEN',\n",
    "  'GRAV_METRO',\n",
    "  'GRAV_SANG',\n",
    "  'GRAV_AST',\n",
    "  'GRAV_MIOC',\n",
    "  'ALRM_HIPOT',\n",
    "  'ALRM_PLAQ',\n",
    "  'ALRM_VOM',\n",
    "  'ALRM_SANG',\n",
    "  'ALRM_HEMAT',\n",
    "  'ALRM_HEPAT',\n",
    "  'DOENCA_TRA',\n",
    "  'RESUL_VI_N',\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "  df[col] = df[col].fillna(2)\n",
    "  df[col] = df[col].replace(2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLio0xsAYY9d"
   },
   "source": [
    "Colocando não realizado para valores nulos do resultados dos exames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "o65dNoWPYUeu"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "  'RES_CHIKS1',\n",
    "  'RES_CHIKS2',\n",
    "  'RESUL_PRNT',\n",
    "  'RESUL_SORO',\n",
    "  'RESUL_NS1',\n",
    "  'IMUNOH_N',\n",
    "  'HISTOPA_N'\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "  df[col] = df[col].fillna(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kKJqModYbQs"
   },
   "source": [
    "Colocando o valor `0` para `CLINIC_CHIK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "DAIZ0wvdYayM"
   },
   "outputs": [],
   "source": [
    "df['CLINC_CHIK'] = df['CLINC_CHIK'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIzU2D6kYe7c"
   },
   "source": [
    "Tratando datas colocando o valor 0 da epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "YGPtpJJRYiN6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
      "/tmp/ipykernel_5086/3444795586.py:18: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n"
     ]
    }
   ],
   "source": [
    "cols = [\n",
    "  'DT_SIN_PRI',\n",
    "  'DT_CHIK_S2',\n",
    "  'DT_PRNT',\n",
    "  'DT_NS1',\n",
    "  'DT_VIRAL',\n",
    "  'DT_PCR',\n",
    "  'DT_OBITO',\n",
    "  'DT_ALRM',\n",
    "  'DT_GRAV',\n",
    "  'DT_INTERNA',\n",
    "  'DT_NOTIFIC'\n",
    "]\n",
    "\n",
    "start_date = pd.to_datetime('1970-01-01')\n",
    "\n",
    "for col in cols:\n",
    "  df[col] = pd.to_datetime(df[col], errors='coerce').view('int64')\n",
    "  df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbl17qt4YgZL"
   },
   "source": [
    "Colocando valores padroes nos campos `CS_*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "W5zxwbfZYlU6"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "  'CS_SEXO',\n",
    "  'CS_GESTANT',\n",
    "  'CS_RACA',\n",
    "  'CS_ESCOL_N',\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "  df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QNq5RntYoqn"
   },
   "source": [
    "Colocando o valor padrao como nao (`2`)  para `HOSPITALIZ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "m6En6YIBYmjx"
   },
   "outputs": [],
   "source": [
    "df['HOSPITALIZ'] = df['HOSPITALIZ'].fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "poTr5zeeYqe3"
   },
   "outputs": [],
   "source": [
    "df['TPAUTOCTO'] = df['TPAUTOCTO'].fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcuEQLrZYx4K"
   },
   "source": [
    "Colocando o valor padrao como `1` para `COPAISINF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "uHi01s4aYwXd"
   },
   "outputs": [],
   "source": [
    "df['COPAISINF'] = df['COPAISINF'].fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cgh9C3tY1PQ"
   },
   "source": [
    "Colocando o valor padrao para `CO_BAINF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "cfxQHL7CYzX2"
   },
   "outputs": [],
   "source": [
    "df['CO_BAINF'] = df['CO_BAINF'].fillna(df['CO_BAINF'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8ESkdKQY4wX"
   },
   "source": [
    "Colocando a moda como valor padrao para `IDENT_MICR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "sH0fOYKLY2y4"
   },
   "outputs": [],
   "source": [
    "df['IDENT_MICR'] = df['IDENT_MICR'].fillna(df['IDENT_MICR'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH9B0tDQY7-n"
   },
   "source": [
    "Agora, podemos verificar que tratamos todas as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1718123849303,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "fCRL_hUkY6hO",
    "outputId": "78a48a36-2de3-4316-f683-32b2b455f0c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1718123849752,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "UpdLlEDyZKAd",
    "outputId": "24520d54-39f4-4d8d-e393-ca22ed8c7627"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NU_NOTIFIC</th>\n",
       "      <th>DT_NOTIFIC</th>\n",
       "      <th>SG_UF_NOT</th>\n",
       "      <th>ID_REGIONA</th>\n",
       "      <th>ID_UNIDADE</th>\n",
       "      <th>DT_SIN_PRI</th>\n",
       "      <th>SEM_PRI</th>\n",
       "      <th>NU_IDADE_N</th>\n",
       "      <th>CS_SEXO</th>\n",
       "      <th>CS_GESTANT</th>\n",
       "      <th>...</th>\n",
       "      <th>GRAV_INSUF</th>\n",
       "      <th>GRAV_HIPOT</th>\n",
       "      <th>GRAV_HEMAT</th>\n",
       "      <th>GRAV_MELEN</th>\n",
       "      <th>GRAV_METRO</th>\n",
       "      <th>GRAV_SANG</th>\n",
       "      <th>GRAV_AST</th>\n",
       "      <th>GRAV_MIOC</th>\n",
       "      <th>DT_GRAV</th>\n",
       "      <th>IDENT_MICR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158</td>\n",
       "      <td>1457136000000000000</td>\n",
       "      <td>29</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>2498731.0</td>\n",
       "      <td>1456876800000000000</td>\n",
       "      <td>201609</td>\n",
       "      <td>3009.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298</td>\n",
       "      <td>1455494400000000000</td>\n",
       "      <td>29</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>3280969.0</td>\n",
       "      <td>1455408000000000000</td>\n",
       "      <td>201607</td>\n",
       "      <td>4039.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5082</td>\n",
       "      <td>1458864000000000000</td>\n",
       "      <td>29</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>2800527.0</td>\n",
       "      <td>1458777600000000000</td>\n",
       "      <td>201612</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111262</td>\n",
       "      <td>1458777600000000000</td>\n",
       "      <td>29</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>2706628.0</td>\n",
       "      <td>1458691200000000000</td>\n",
       "      <td>201612</td>\n",
       "      <td>4065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>1457827200000000000</td>\n",
       "      <td>29</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>2498731.0</td>\n",
       "      <td>1457740800000000000</td>\n",
       "      <td>201610</td>\n",
       "      <td>4067.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NU_NOTIFIC           DT_NOTIFIC  SG_UF_NOT  ID_REGIONA  ID_UNIDADE  \\\n",
       "0         158  1457136000000000000         29      1381.0   2498731.0   \n",
       "1         298  1455494400000000000         29      1385.0   3280969.0   \n",
       "2        5082  1458864000000000000         29      1385.0   2800527.0   \n",
       "3      111262  1458777600000000000         29      1385.0   2706628.0   \n",
       "4         166  1457827200000000000         29      1381.0   2498731.0   \n",
       "\n",
       "            DT_SIN_PRI  SEM_PRI  NU_IDADE_N  CS_SEXO  CS_GESTANT  ...  \\\n",
       "0  1456876800000000000   201609      3009.0      1.0         6.0  ...   \n",
       "1  1455408000000000000   201607      4039.0      1.0         6.0  ...   \n",
       "2  1458777600000000000   201612      4053.0      0.0         5.0  ...   \n",
       "3  1458691200000000000   201612      4065.0      0.0         6.0  ...   \n",
       "4  1457740800000000000   201610      4067.0      0.0         6.0  ...   \n",
       "\n",
       "   GRAV_INSUF  GRAV_HIPOT  GRAV_HEMAT  GRAV_MELEN  GRAV_METRO  GRAV_SANG  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0        0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0        0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0        0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0        0.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0        0.0   \n",
       "\n",
       "   GRAV_AST  GRAV_MIOC              DT_GRAV  IDENT_MICR  \n",
       "0       0.0        0.0 -9223372036854775808         4.0  \n",
       "1       0.0        0.0 -9223372036854775808         4.0  \n",
       "2       0.0        0.0 -9223372036854775808         4.0  \n",
       "3       0.0        0.0 -9223372036854775808         4.0  \n",
       "4       0.0        0.0 -9223372036854775808         4.0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPFYwkf0ZAZh"
   },
   "source": [
    "## Exportando os dados\n",
    "\n",
    "vamos exportar os dados para `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1718123849753,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "X3NwUOk6lj-5",
    "outputId": "34fbc544-6681-4125-af13-ab92c67185c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1457136000000000000\n",
       "1    1455494400000000000\n",
       "2    1458864000000000000\n",
       "3    1458777600000000000\n",
       "4    1457827200000000000\n",
       "Name: DT_NOTIFIC, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DT_NOTIFIC'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "HHxAr6cnccrJ"
   },
   "outputs": [],
   "source": [
    "df_tmp = df.copy()\n",
    "\n",
    "cols = [\n",
    "  'DT_SIN_PRI',\n",
    "  'DT_CHIK_S2',\n",
    "  'DT_PRNT',\n",
    "  'DT_NS1',\n",
    "  'DT_VIRAL',\n",
    "  'DT_PCR',\n",
    "  'DT_OBITO',\n",
    "  'DT_ALRM',\n",
    "  'DT_GRAV',\n",
    "  'DT_INTERNA',\n",
    "  'DT_NOTIFIC'\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "  df_tmp[col] = pd.to_datetime(df_tmp[col], errors='coerce', unit='ns')\n",
    "\n",
    "df_tmp.to_csv('out.csv', index=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1718123881056,
     "user": {
      "displayName": "Carlos Eduardo da Silva Cerqueira",
      "userId": "07090745159977828532"
     },
     "user_tz": 180
    },
    "id": "M6McHGyQkHYj",
    "outputId": "7fc294a0-fadf-4b20-9166-fab37b7254a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NU_NOTIFIC</th>\n",
       "      <th>DT_NOTIFIC</th>\n",
       "      <th>SG_UF_NOT</th>\n",
       "      <th>ID_REGIONA</th>\n",
       "      <th>ID_UNIDADE</th>\n",
       "      <th>DT_SIN_PRI</th>\n",
       "      <th>SEM_PRI</th>\n",
       "      <th>NU_IDADE_N</th>\n",
       "      <th>CS_SEXO</th>\n",
       "      <th>CS_GESTANT</th>\n",
       "      <th>...</th>\n",
       "      <th>GRAV_INSUF</th>\n",
       "      <th>GRAV_HIPOT</th>\n",
       "      <th>GRAV_HEMAT</th>\n",
       "      <th>GRAV_MELEN</th>\n",
       "      <th>GRAV_METRO</th>\n",
       "      <th>GRAV_SANG</th>\n",
       "      <th>GRAV_AST</th>\n",
       "      <th>GRAV_MIOC</th>\n",
       "      <th>DT_GRAV</th>\n",
       "      <th>IDENT_MICR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158</td>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>29</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>2498731.0</td>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>201609</td>\n",
       "      <td>3009.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298</td>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>29</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>3280969.0</td>\n",
       "      <td>2016-02-14</td>\n",
       "      <td>201607</td>\n",
       "      <td>4039.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5082</td>\n",
       "      <td>2016-03-25</td>\n",
       "      <td>29</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>2800527.0</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>201612</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111262</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>29</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>2706628.0</td>\n",
       "      <td>2016-03-23</td>\n",
       "      <td>201612</td>\n",
       "      <td>4065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>29</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>2498731.0</td>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>201610</td>\n",
       "      <td>4067.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NU_NOTIFIC DT_NOTIFIC  SG_UF_NOT  ID_REGIONA  ID_UNIDADE DT_SIN_PRI  \\\n",
       "0         158 2016-03-05         29      1381.0   2498731.0 2016-03-02   \n",
       "1         298 2016-02-15         29      1385.0   3280969.0 2016-02-14   \n",
       "2        5082 2016-03-25         29      1385.0   2800527.0 2016-03-24   \n",
       "3      111262 2016-03-24         29      1385.0   2706628.0 2016-03-23   \n",
       "4         166 2016-03-13         29      1381.0   2498731.0 2016-03-12   \n",
       "\n",
       "   SEM_PRI  NU_IDADE_N  CS_SEXO  CS_GESTANT  ...  GRAV_INSUF  GRAV_HIPOT  \\\n",
       "0   201609      3009.0      1.0         6.0  ...         0.0         0.0   \n",
       "1   201607      4039.0      1.0         6.0  ...         0.0         0.0   \n",
       "2   201612      4053.0      0.0         5.0  ...         0.0         0.0   \n",
       "3   201612      4065.0      0.0         6.0  ...         0.0         0.0   \n",
       "4   201610      4067.0      0.0         6.0  ...         0.0         0.0   \n",
       "\n",
       "   GRAV_HEMAT  GRAV_MELEN  GRAV_METRO  GRAV_SANG  GRAV_AST  GRAV_MIOC  \\\n",
       "0         0.0         0.0         0.0        0.0       0.0        0.0   \n",
       "1         0.0         0.0         0.0        0.0       0.0        0.0   \n",
       "2         0.0         0.0         0.0        0.0       0.0        0.0   \n",
       "3         0.0         0.0         0.0        0.0       0.0        0.0   \n",
       "4         0.0         0.0         0.0        0.0       0.0        0.0   \n",
       "\n",
       "   DT_GRAV  IDENT_MICR  \n",
       "0      NaT         4.0  \n",
       "1      NaT         4.0  \n",
       "2      NaT         4.0  \n",
       "3      NaT         4.0  \n",
       "4      NaT         4.0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "EHi3O2lzd5Ae"
   },
   "outputs": [],
   "source": [
    "df.to_csv('out_brute.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7jdvEAFvB-m"
   },
   "source": [
    "# Treinamento\n",
    "\n",
    "Para facilitar o processo de treinamento, vamos criar uma classe `TrainingModels` responsavel por realizar aplicar e realizar o treinamento dos modelos selecionados. Podemos alterar os valores dos parametros durante a inicializacao do objeto.\n",
    "\n",
    "Estão sendo utilizados os seguintes modelos:\n",
    "KNeighborsClassifier, DecisionTreeClassifier, RandomForestClassifier, LogisticRegression e MLPClassifier\n",
    "\n",
    "Respectivos hiperparâmetros estão setados no construtor da classe:\n",
    " `knn_neighbors = 5`, `knn_weights = 'uniform'`, `knn_metric = 'euclidean'`, `dt_criterion = 'entropy'`, `dt_min_samples_split = 2`,`rf_n_estimators = 100`, `rf_criterion = 'entropy'`,`logistic_max_iter = 100`, `logistic_penalty = 'l2'`,`logistic_solver = 'lbfgs'`, `mlp_hidden_layer_sizes = (100, 100)`, `mlp_activation = 'relu'`, `mlp_solver = 'adam'`,\n",
    "`mlp_learning_rate_init = 0.001`, `mlp_max_iter = 200`,`mlp_batch_size = 32`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Ds5J2Y0bpmwt"
   },
   "outputs": [],
   "source": [
    "class TrainingModels:\n",
    "  def __init__(\n",
    "    self,\n",
    "    knn_neighbors: int = 5,\n",
    "    knn_weights: str = 'uniform',\n",
    "    knn_metric: str = 'euclidean',\n",
    "    dt_criterion: str = 'entropy',\n",
    "    dt_min_samples_split: int = 2,\n",
    "    rf_n_estimators: int = 100,\n",
    "    rf_criterion: str = 'entropy',\n",
    "    rf_max_depth: int = None,\n",
    "    logistic_max_iter: int = 100,\n",
    "    logistic_penalty: str = 'l2',\n",
    "    logistic_solver: str = 'lbfgs',\n",
    "    mlp_hidden_layer_sizes: tuple[int, int] = (100, 100),\n",
    "    mlp_activation: str = 'relu',\n",
    "    mlp_solver: str = 'adam',\n",
    "    mlp_learning_rate_init: float = 0.001,\n",
    "    mlp_max_iter: int = 200,\n",
    "    mlp_batch_size: int = 32\n",
    "  ):\n",
    "    self.knn = KNeighborsClassifier(\n",
    "        n_neighbors=knn_neighbors,\n",
    "        weights=knn_weights,\n",
    "        metric=knn_metric\n",
    "      )\n",
    "    self.dt = DecisionTreeClassifier(\n",
    "        criterion=dt_criterion,\n",
    "        min_samples_split=dt_min_samples_split\n",
    "      )\n",
    "    self.rf = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        criterion=rf_criterion,\n",
    "        max_depth=rf_max_depth\n",
    "      )\n",
    "    self.logistic = LogisticRegression(\n",
    "        max_iter=logistic_max_iter,\n",
    "        penalty=logistic_penalty,\n",
    "        solver=logistic_solver\n",
    "      )\n",
    "    self.mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=mlp_hidden_layer_sizes,\n",
    "        activation=mlp_activation,\n",
    "        solver=mlp_solver,\n",
    "        learning_rate_init=mlp_learning_rate_init,\n",
    "        max_iter=mlp_max_iter,\n",
    "        batch_size=mlp_batch_size\n",
    "      )\n",
    "\n",
    "  def fit(self, X_train: pd.DataFrame, y_train: pd.DataFrame) -> None:\n",
    "    print(\"Treinando Knn...\")\n",
    "    self.knn.fit(X_train, y_train)\n",
    "    print(\"Treinando DT...\")\n",
    "    self.dt.fit(X_train, y_train)\n",
    "    print(\"Treinando RF...\")\n",
    "    self.rf.fit(X_train, y_train)\n",
    "    print(\"Treinando Logistic...\")\n",
    "    self.logistic.fit(X_train, y_train)\n",
    "    print(\"Treinando MLP\")\n",
    "    self.mlp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "  def plot_metrics(self, X_train, y_train, X_test, y_test):\n",
    "        models = {'KNN': self.knn, 'Decision Tree': self.dt, 'Random Forest': self.rf, 'Logistic Regression': self.logistic, 'MLP': self.mlp}\n",
    "        #models = {'KNN': self.knn, 'Decision Tree': self.dt, 'Random Forest': self.rf, 'Logistic Regression': self.logistic}\n",
    "        metrics = {'Accuracy': accuracy_score, 'Precision': precision_score, 'Recall': recall_score, 'F1 Score': f1_score}\n",
    "        colors = ['b', 'g', 'r', 'c', 'm']\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bar_width = 0.15\n",
    "        index = np.arange(len(models))\n",
    "        metric_values = {}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            metric_values[model_name] = [metric_func(y_test, y_pred) for metric_func in metrics.values()]\n",
    "\n",
    "            # Print confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "            plt.title(f'{model_name} Confusion Matrix')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.show()\n",
    "\n",
    "        for i, (metric_name, metric_color) in enumerate(zip(metrics.keys(), colors)):\n",
    "            plt.bar(index + i * bar_width, [metric_value[i] for metric_value in metric_values.values()], bar_width, label=metric_name, color=metric_color)\n",
    "\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Model Metrics Comparison')\n",
    "        plt.xticks(index + bar_width * (len(metrics) - 1) / 2, models.keys())\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "  def load_models(self, knn: str, dt: str, rf: str, logistic: str, mlp: str):\n",
    "    self.knn = sio.load(knn)\n",
    "    self.dt = sio.load(dt)\n",
    "    self.rf = sio.load(rf)\n",
    "    self.logistic = sio.load(logistic)\n",
    "    self.mlp = sio.load(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "sG78qSMivFPv"
   },
   "outputs": [],
   "source": [
    "models = TrainingModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ER3KKOXQvIzi"
   },
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=df.columns)\n",
    "\n",
    "X = df_scaled.drop([\"CLASSI_FIN\"], axis=1)\n",
    "y = df_scaled['CLASSI_FIN']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_we7nH_vOf0",
    "outputId": "0b306686-8fac-423e-d9f9-49e4ffd8a38a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando Knn...\n",
      "Treinando DT...\n",
      "Treinando RF...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 56\u001b[0m, in \u001b[0;36mTrainingModels.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTreinando RF...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTreinando Logistic...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogistic\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzaUOcEIJWOF"
   },
   "source": [
    "Métricas avaliadas:\n",
    "\n",
    "- 'Accuracy',\n",
    "- 'Precision',\n",
    "- 'Recall',\n",
    "- 'F1 Score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJsVT-vsvQrW"
   },
   "outputs": [],
   "source": [
    "models.plot_metrics(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkbTR3MUWzIO"
   },
   "source": [
    "Assim, podemos ver que os algoritmos de `DecisionTree` e, principalmente, `RandomForest` sao os melhores algoritmos para o problema proposto. O que se encaixa com o que seria esperado para esse tipo de problema.\n",
    "\n",
    "Como possivel melhoria do sistema, seria possivel aumentar a quatidade de interacoes da regressao logistica na esperanca de termos um melhor resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model(models.mlp, \"mlp_pesos_padroes\")\n",
    "export_model(models.logistic, \"logistic_pesos_padroes\")\n",
    "export_model(models.knn, \"knn_pesos_padroes\")\n",
    "export_model(models.dt, \"dt_pesos_padroes\")\n",
    "export_model(models.rf, \"rf_pesos_padroes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecionando Parametros\n",
    "\n",
    "Vamos usar `RandomizedSearchCV` para tentar encontrar os melhores paremetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_dist, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# pegar por f1, curva rock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asssim, temos que os melhores valores sao:\n",
    "\n",
    "- `n_estimators`: 100\n",
    "- `max_depth`: None\n",
    "- `criterion`: entropy\n",
    "- `bootstrap`: True\n",
    "- `class_weight`: balanced\n",
    "- `max_features`: sqrt\n",
    "- `min_samples_leaf`: 1\n",
    "- `min_samples_split`: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_neighbors': [10, 20, 25, 30, 35, 40],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=param_dist, scoring='accuracy', random_state=42, verbose = 1)\n",
    "random_search.fit(X, y)\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asssim, temos que os melhores valores sao:\n",
    "\n",
    "- `n_neighbors`: 35\n",
    "- `weights`: distance\n",
    "- `metric`: manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m]\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(DecisionTreeClassifier(), param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m, y)\n\u001b[1;32m      7\u001b[0m best_params \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [5, 10, 20]\n",
    "}\n",
    "random_search = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=param_dist, scoring='accuracy', random_state=42, verbose = 1)\n",
    "random_search.fit(X, y)\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asssim, temos que os melhores valores sao:\n",
    "\n",
    "- `criterion`: entropy\n",
    "- `min_samples_split`: 10\n",
    "- `splitter`: random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'penalty': ['l2', 'elasticnet', None],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'max_iter': [350, 400, 450, 500, 550, 600, 700, 800]\n",
    "}\n",
    "random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist, scoring='accuracy', random_state=42, verbose = 1)\n",
    "random_search.fit(X, y)\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asssim, temos que os melhores valores sao:\n",
    "\n",
    "- `penalty`: l2\n",
    "- `solver`: lbfgs\n",
    "- `max_iter`: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(100, 100), (150, 150)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam', 'sgd', 'lbfgs'],\n",
    "    'learning_rate_init': [0.0001, 0.001],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'batch_size': [16, 32]\n",
    "}\n",
    "random_search = RandomizedSearchCV(MLPClassifier(), param_distributions=param_dist, scoring='accuracy', random_state=42, verbose = 1)\n",
    "random_search.fit(X, y)\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asssim, temos que os melhores valores sao:\n",
    "\n",
    "- `hidden_layer_sizes`: (150, 150)\n",
    "- `activation`: relu\n",
    "- `solver`: adam\n",
    "- `learning_rate_init`: 0.001\n",
    "- `max_iter`: 300\n",
    "- `batch_size`: 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models = TrainingModels(\n",
    "    rf_n_estimators = 100,\n",
    "    rf_criterion = 'entropy',\n",
    "    rf_max_depth = None,\n",
    "    knn_neighbors = 35,\n",
    "    knn_weights = 'distance',\n",
    "    knn_metric = 'manhattan',\n",
    "    dt_criterion = 'entropy',\n",
    "    dt_min_samples_split = 10,\n",
    "    logistic_max_iter = 200,\n",
    "    logistic_penalty = 'l2',\n",
    "    logistic_solver = 'lbfgs',\n",
    "    mlp_hidden_layer_sizes = (150, 150),\n",
    "    mlp_activation = 'relu',\n",
    "    mlp_solver = 'adam',\n",
    "    mlp_learning_rate_init = 0.001,\n",
    "    mlp_max_iter = 300,\n",
    "    mlp_batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando Knn...\n",
      "Treinando DT...\n",
      "Treinando RF...\n",
      "Treinando Logistic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/.platformio/penv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando MLP\n"
     ]
    }
   ],
   "source": [
    "new_models.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models.plot_metrics(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model(new_models.mlp, \"mlp_parametros_livres\")\n",
    "export_model(new_models.logistic, \"logistic_parametros_livres\")\n",
    "export_model(new_models.knn, \"knn_parametros_livres\")\n",
    "export_model(new_models.dt, \"dt_parametros_livres\")\n",
    "export_model(new_models.rf, \"rf_parametros_livres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando os modelos exportados\n",
    "\n",
    "Podemos usar os modelos que exportamos a cima para treinar o uso de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "UntrustedTypesFoundException",
     "evalue": "Untrusted types found in the file: ['sklearn.neural_network._stochastic_optimizers.AdamOptimizer'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUntrustedTypesFoundException\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m TrainingModels()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./knn_parametros_livres.skops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dt_parametros_livres.skops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./rf_parametros_livres.skops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlogistic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logistic_parametros_livres.skops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./mlp_parametros_livres.skops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 104\u001b[0m, in \u001b[0;36mTrainingModels.load_models\u001b[0;34m(self, knn, dt, rf, logistic, mlp)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrf \u001b[38;5;241m=\u001b[39m sio\u001b[38;5;241m.\u001b[39mload(rf)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogistic \u001b[38;5;241m=\u001b[39m sio\u001b[38;5;241m.\u001b[39mload(logistic)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43msio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/skops/io/_persist.py:151\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, trusted)\u001b[0m\n\u001b[1;32m    149\u001b[0m     load_context \u001b[38;5;241m=\u001b[39m LoadContext(src\u001b[38;5;241m=\u001b[39minput_zip, protocol\u001b[38;5;241m=\u001b[39mschema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    150\u001b[0m     tree \u001b[38;5;241m=\u001b[39m get_tree(schema, load_context, trusted\u001b[38;5;241m=\u001b[39mtrusted)\n\u001b[0;32m--> 151\u001b[0m     \u001b[43maudit_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     instance \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mconstruct()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[0;32m~/.platformio/penv/lib/python3.12/site-packages/skops/io/_audit.py:59\u001b[0m, in \u001b[0;36maudit_tree\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m     57\u001b[0m unsafe \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mget_unsafe_set()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsafe:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UntrustedTypesFoundException(unsafe)\n",
      "\u001b[0;31mUntrustedTypesFoundException\u001b[0m: Untrusted types found in the file: ['sklearn.neural_network._stochastic_optimizers.AdamOptimizer']."
     ]
    }
   ],
   "source": [
    "models = TrainingModels()\n",
    "models.load_models(knn=\"./knn_parametros_livres.skops\",\n",
    "                   dt=\"./dt_parametros_livres.skops\",\n",
    "                   rf=\"./rf_parametros_livres.skops\",\n",
    "                   logistic=\"./logistic_parametros_livres.skops\",\n",
    "                   mlp=\"./mlp_parametros_livres.skops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
